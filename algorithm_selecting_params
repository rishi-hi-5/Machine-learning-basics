#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jul 30 17:37:09 2017

@author: rishi
"""

#R squared 
#we used ordinary least square method to found our model or best fitting line for linear regression . the formula used is
#sum of squared residues (SSres)=Sum((y-y`)^2) where y is actual value and y` is predicted value
# there is another term which can be obtained if we use the average value of y instead if predicted values. this leads us to new term
#total sum of squared (SStot)=Sum((y-yavg)^2)
#once we have these two value we can calculate Rsquared which is given by 
#Rsquared=1-(SSres/SStot)
#significance :: it tells us how good is the line compared to the average line.
#the closer Rsquared to 1 the bettwe is the model
#Rsquared can be negative however this means the model is broken
#R^2 gives goodness of fit
#What is adjusted Rsquared?
#R square tells if it is ok to have variables 
#R^2 will never decrease by addition of variable. it will always increase . if addition of variable does not increase R^2 then dont consider the variable for the model

#Ajd R^2 = 1-(1-R^2)(n-1/n-p-1)
#where p - number of regressor or independent variables and n is sample size
#adjusted R^2 has penalization factor meaning it will cost you for adding the extra variables that are not good for the model

#cofficients in results 
#positive coefficient means the dependent variable is directly proportional to that particular independent variable . Magnitude is tricky thing . always consider the magnitude of coefficient in terms of per unit of dependent variable . if two independent variables are of same unit then you can compare there magnitude else you cannot compare the magnitude . we check only for the a unit increase of dependent variable is dependent on  unit increase of a independent variable when other variables are constant.coefficents tells about the effect that the variable brings in the model it changes with addition and deletion of the variable